{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f05cc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638a188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv('/home/dataguy/hobby_project/joined_data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a1366",
   "metadata": {},
   "source": [
    "## I always like to have a look at the dataset, what it looks like, what datatypes it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce539f4",
   "metadata": {},
   "source": [
    "## For the .csv file I manually wrote the latitude and longitude columns, which I could have done with code but given the short range of the dataset this was faster for me.\n",
    "## There was a population column in the dataset which I got rid of using bash ( I like to do the data cleaning in bash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14b76ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>41.3275</td>\n",
       "      <td>19.81890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>42.5063</td>\n",
       "      <td>1.52180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Vienna</td>\n",
       "      <td>48.2085</td>\n",
       "      <td>16.37210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Belarus</td>\n",
       "      <td>Minsk</td>\n",
       "      <td>53.9000</td>\n",
       "      <td>27.56670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>Brussels</td>\n",
       "      <td>50.8504</td>\n",
       "      <td>4.34878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country              City      Lat       Lng\n",
       "0  Albania            Tirana  41.3275  19.81890\n",
       "1  Andorra  Andorra la Vella  42.5063   1.52180\n",
       "2  Austria            Vienna  48.2085  16.37210\n",
       "3  Belarus             Minsk  53.9000  27.56670\n",
       "4  Belgium          Brussels  50.8504   4.34878"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af23728",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries and loading the API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2fc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca076fc8",
   "metadata": {},
   "source": [
    "## I'm going to keep this part here, because I like to experiment with the code first and take a look of what it will look like and how should I move on from here.\n",
    "## In this part I load the request and print it into a .json file.\n",
    "###  I changed the API key in the public version for safety reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21322ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://api.openweathermap.org/data/2.5/air_pollution/history?lat=47.4983&lon=19.0404&start=1685570400&end=1685584800&appid=12345'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2491e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9e9fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_data = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ad364cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coord': {'lon': 19.0404, 'lat': 47.4983},\n",
       " 'list': [{'main': {'aqi': 2},\n",
       "   'components': {'co': 377.18,\n",
       "    'no': 1.15,\n",
       "    'no2': 40.44,\n",
       "    'o3': 8.23,\n",
       "    'so2': 4.11,\n",
       "    'pm2_5': 14.45,\n",
       "    'pm10': 19.21,\n",
       "    'nh3': 5.26},\n",
       "   'dt': 1685570400},\n",
       "  {'main': {'aqi': 2},\n",
       "   'components': {'co': 317.1,\n",
       "    'no': 0.1,\n",
       "    'no2': 27.42,\n",
       "    'o3': 19.85,\n",
       "    'so2': 3.28,\n",
       "    'pm2_5': 12.18,\n",
       "    'pm10': 15.9,\n",
       "    'nh3': 4.43},\n",
       "   'dt': 1685574000},\n",
       "  {'main': {'aqi': 2},\n",
       "   'components': {'co': 283.72,\n",
       "    'no': 0.03,\n",
       "    'no2': 20.39,\n",
       "    'o3': 26.46,\n",
       "    'so2': 3.16,\n",
       "    'pm2_5': 11.26,\n",
       "    'pm10': 14.38,\n",
       "    'nh3': 3.8},\n",
       "   'dt': 1685577600},\n",
       "  {'main': {'aqi': 2},\n",
       "   'components': {'co': 263.69,\n",
       "    'no': 0.03,\n",
       "    'no2': 17.65,\n",
       "    'o3': 24.32,\n",
       "    'so2': 2.77,\n",
       "    'pm2_5': 10.95,\n",
       "    'pm10': 13.85,\n",
       "    'nh3': 3.04},\n",
       "   'dt': 1685581200},\n",
       "  {'main': {'aqi': 2},\n",
       "   'components': {'co': 253.68,\n",
       "    'no': 0.1,\n",
       "    'no2': 17.14,\n",
       "    'o3': 17.88,\n",
       "    'so2': 2.44,\n",
       "    'pm2_5': 10.95,\n",
       "    'pm10': 13.8,\n",
       "    'nh3': 2.75},\n",
       "   'dt': 1685584800}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pollution_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43d370",
   "metadata": {},
   "source": [
    "## This code below specifies the file path and saves the file in .json format.\n",
    "## This is also just a little experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7acb637",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/dataguy/hobby_project/full_pollution_data.json'\n",
    "\n",
    "json_data = json.dumps(pollution_data)\n",
    "\n",
    "with open('/home/dataguy/hobby_project/full_pollution_data.json', 'w') as file:\n",
    "    file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eba9a79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main': {'aqi': 3}, 'components': {'co': 393.87, 'no': 0.67, 'no2': 20.74, 'o3': 6.26, 'so2': 1.48, 'pm2_5': 20.9, 'pm10': 24.74, 'nh3': 0.66}, 'dt': 1609459200}\n",
      "{'main': {'aqi': 3}, 'components': {'co': 397.21, 'no': 0.89, 'no2': 20.39, 'o3': 5.54, 'so2': 1.42, 'pm2_5': 21.75, 'pm10': 25.53, 'nh3': 0.7}, 'dt': 1609462800}\n",
      "{'main': {'aqi': 3}, 'components': {'co': 400.54, 'no': 1.24, 'no2': 20.05, 'o3': 4.96, 'so2': 1.86, 'pm2_5': 22.34, 'pm10': 26.04, 'nh3': 1.24}, 'dt': 1609466400}\n",
      "{'main': {'aqi': 3}, 'components': {'co': 397.21, 'no': 1.31, 'no2': 19.02, 'o3': 5.32, 'so2': 2.83, 'pm2_5': 22.41, 'pm10': 25.97, 'nh3': 2.5}, 'dt': 1609470000}\n",
      "{'main': {'aqi': 3}, 'components': {'co': 393.87, 'no': 1.15, 'no2': 18.34, 'o3': 6.62, 'so2': 3.37, 'pm2_5': 22.19, 'pm10': 25.48, 'nh3': 3.01}, 'dt': 1609473600}\n",
      "{'main': {'aqi': 3}, 'components': {'co': 403.88, 'no': 1.15, 'no2': 18.85, 'o3': 7.69, 'so2': 3.93, 'pm2_5': 22.92, 'pm10': 25.93, 'nh3': 3.1}, 'dt': 1609477200}\n",
      "{'main': {'aqi': 4}, 'components': {'co': 453.95, 'no': 2.71, 'no2': 23.31, 'o3': 5.19, 'so2': 5.25, 'pm2_5': 26.59, 'pm10': 29.86, 'nh3': 3.55}, 'dt': 1609480800}\n",
      "{'main': {'aqi': 4}, 'components': {'co': 480.65, 'no': 3.86, 'no2': 25.71, 'o3': 3.53, 'so2': 5.78, 'pm2_5': 27.07, 'pm10': 30.79, 'nh3': 3.93}, 'dt': 1609484400}\n",
      "{'main': {'aqi': 3}, 'components': {'co': 467.3, 'no': 5.64, 'no2': 22.62, 'o3': 6.53, 'so2': 4.35, 'pm2_5': 24.41, 'pm10': 28.89, 'nh3': 2.06}, 'dt': 1609488000}\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file\n",
    "with open('/home/dataguy/hobby_project/full_pollution_data.json') as file:\n",
    "    # Load the JSON data\n",
    "    data = json.load(file)\n",
    "\n",
    "# Now you can work with the JSON data as a Python object\n",
    "for i in range(1,10):\n",
    "    dt_value = data['list'][i]\n",
    "    print(dt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c0e15",
   "metadata": {},
   "source": [
    "# I had to convert the .json files into .csv files so I can work with them and load them into SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9369851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "json_file = '43_cities.json'\n",
    "csv_file = '43_cities.csv'\n",
    "field_names = ['lon', 'lat', 'aqi', 'co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'dt']\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(json_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(field_names)\n",
    "\n",
    "    # Write the data row\n",
    "    for location_key, location_data in data.items():\n",
    "        lon, lat = map(float, location_key.split(','))  # Extract the lon and lat values from the key\n",
    "\n",
    "        for item in location_data['list']:\n",
    "            writer.writerow([lon,\n",
    "                             lat,\n",
    "                             item['main']['aqi'],\n",
    "                             item['components']['co'],\n",
    "                             item['components']['no'],\n",
    "                             item['components']['no2'],\n",
    "                             item['components']['o3'],\n",
    "                             item['components']['so2'],\n",
    "                             item['components']['pm2_5'],\n",
    "                             item['components']['pm10'],\n",
    "                             item['components']['nh3'],\n",
    "                             item['dt']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30787cea",
   "metadata": {},
   "source": [
    "## Here I load the API requests and store them in a .jason file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bc5481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = 'http://api.openweathermap.org/data/2.5/air_pollution/history?lat={lat}&lon={lon}&start=1609459200&end=1685574000&appid=12345'\n",
    "\n",
    "locations = [\n",
    "    (44.8040, 20.4651),\n",
    "    (48.1482, 17.1067),\n",
    "    (46.0511, 14.5051),\n",
    "    (40.4165, -3.7025),\n",
    "    (59.3294, 18.0687),\n",
    "    (46.9480, 7.4474),\n",
    "    (50.4501, 30.5234),\n",
    "    (51.5085, -0.1257)\n",
    "    # Add more latitude and longitude values as needed\n",
    "]\n",
    "\n",
    "# Create a dictionary to store the API responses\n",
    "data = {}\n",
    "\n",
    "# Loop through the locations and make API requests\n",
    "for lat, lon in locations:\n",
    "    url = base_url.format(lat=lat, lon=lon)\n",
    "    response = requests.get(url)\n",
    "    response_data = response.json()\n",
    "\n",
    "    # Convert the latitude and longitude tuple to a string for use as the dictionary key\n",
    "    key = f\"{lat},{lon}\"\n",
    "\n",
    "    # Store the response data using the string key\n",
    "    data[key] = response_data\n",
    "\n",
    "# Save the data dictionary into a JSON file\n",
    "with open('43_cities.json', 'w') as file:\n",
    "    json.dump(data, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88e494",
   "metadata": {},
   "source": [
    "# This part opens the .csv file, then takes the 'dt' column, which is in unix timestamp format, it stores the data in a temporary list and then it formats the values as readable timestamp format. After it updates, the values are written back into the .csv file but now in formatted datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a1bcc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "\n",
    "csv_file = '43_cities.csv'\n",
    "\n",
    "# Create a temporary list to hold the updated rows\n",
    "updated_rows = []\n",
    "\n",
    "# Open the CSV file\n",
    "with open(csv_file, 'r') as file:\n",
    "    # Create a CSV reader\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    # Read and store the header row\n",
    "    header_row = next(reader)\n",
    "    updated_rows.append(header_row)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # Extract the Unix timestamp from the 'dt' column in the row\n",
    "        unix_timestamp = int(row[11])\n",
    "\n",
    "        # Convert the Unix timestamp to a datetime object\n",
    "        datetime_obj = datetime.datetime.fromtimestamp(unix_timestamp)\n",
    "\n",
    "        # Format the datetime object as a string in the desired format\n",
    "        formatted_datetime = datetime_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Update the 'dt' column in the row with the formatted datetime\n",
    "        row[11] = formatted_datetime\n",
    "\n",
    "        # Add the updated row to the list\n",
    "        updated_rows.append(row)\n",
    "\n",
    "# Write the updated rows back to the same CSV file\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    # Create a CSV writer\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the updated rows\n",
    "    writer.writerows(updated_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
